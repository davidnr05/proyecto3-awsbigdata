{"changed":true,"filter":false,"title":"job_scrape_html.py","tooltip":"/headlines_downloader/scripts/job_scrape_html.py","value":"import requests\nimport boto3\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n#rrrr\ndef scrape_headlines(event=None, context=None):\n    urls = {\n        \"eltiempo\": \"https://www.eltiempo.com/\",\n        \"publimetro\": \"https://www.publimetro.co/\"\n    }\n\n    s3 = boto3.client(\"s3\")\n    bucket = \"headlinesjune\"  # ← Estaba mal cerrada la comilla\n    today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n    year, month, day = today.split(\"-\")\n\n    headers = {\n        \"User-Agent\": (\n            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n            \"Chrome/114.0.0.0 Safari/537.36\"\n        ),\n        \"Accept\": (\n            \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\"\n        ),\n        \"Accept-Language\": \"es-ES,es;q=0.9\",\n        \"Referer\": \"https://www.google.com/\",\n        \"Connection\": \"keep-alive\",\n        \"Upgrade-Insecure-Requests\": \"1\"\n    }\n\n    for nombre, url in urls.items():\n        # 1. Descargar HTML\n        response = requests.get(url, headers=headers, timeout=10)\n        html = response.content.decode(\"utf-8\")\n\n        # 2. Guardar HTML en S3\n        raw_key = f\"headlines/raw/{nombre}-{today}.html\"\n        s3.put_object(\n            Bucket=bucket,\n            Key=raw_key,\n            Body=response.content,\n            ContentType='text/html'\n        )\n\n        # 3. Procesar con BeautifulSoup\n        soup = BeautifulSoup(html, 'html.parser')\n        noticias = []\n\n        if nombre == \"eltiempo\":\n            for a in soup.select(\"article a[href]\"):\n                titulo = a.get_text(strip=True)\n                enlace = a.get(\"href\")\n                if enlace and not enlace.startswith(\"http\"):\n                    enlace = \"https://www.eltiempo.com\" + enlace\n                if titulo and enlace:\n                    noticias.append([\"General\", titulo, enlace])\n\n        elif nombre == \"publimetro\":\n            for link in soup.select(\"a[href]\"):\n                titulo = link.get_text(strip=True)\n                enlace = link.get(\"href\")\n                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:\n                    if not enlace.startswith(\"http\"):\n                        enlace = \"https://www.publimetro.co\" + enlace\n                    noticias.append([\"General\", titulo, enlace])\n\n        # 4. Crear CSV en memoria\n        csv = \"categoria,titulo,enlace\\n\"\n        for row in noticias:\n            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"\n\n        # 5. Guardar CSV en S3\n        final_key = f\"headlines/final/periodico={nombre}/year={year}/month={month}/day={day}/noticias.csv\"\n        s3.put_object(\n            Bucket=bucket,\n            Key=final_key,\n            Body=csv.encode(\"utf-8\"),\n            ContentType=\"text/csv\"\n        )\n\n    return {\"message\": \"HTML descargados y procesados correctamente\"}\n","undoManager":{"mark":-2,"position":0,"stack":[[{"start":{"row":0,"column":0},"end":{"row":82,"column":0},"action":"remove","lines":["import requests","import boto3","from bs4 import BeautifulSoup","from datetime import datetime","","def scrape_headlines(event=None, context=None):","    urls = {","        \"eltiempo\": \"https://www.eltiempo.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    s3 = boto3.client(\"s3\")","    bucket = \"headlinesjune\"  # ← Estaba mal cerrada la comilla","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","    year, month, day = today.split(\"-\")","","    headers = {","        \"User-Agent\": (","            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"","            \"AppleWebKit/537.36 (KHTML, like Gecko) \"","            \"Chrome/114.0.0.0 Safari/537.36\"","        ),","        \"Accept\": (","            \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\"","        ),","        \"Accept-Language\": \"es-ES,es;q=0.9\",","        \"Referer\": \"https://www.google.com/\",","        \"Connection\": \"keep-alive\",","        \"Upgrade-Insecure-Requests\": \"1\"","    }","","    for nombre, url in urls.items():","        # 1. Descargar HTML","        response = requests.get(url, headers=headers, timeout=10)","        html = response.content.decode(\"utf-8\")","","        # 2. Guardar HTML en S3","        raw_key = f\"headlines/raw/{nombre}-{today}.html\"","        s3.put_object(","            Bucket=bucket,","            Key=raw_key,","            Body=response.content,","            ContentType='text/html'","        )","","        # 3. Procesar con BeautifulSoup","        soup = BeautifulSoup(html, 'html.parser')","        noticias = []","","        if nombre == \"eltiempo\":","            for a in soup.select(\"article a[href]\"):","                titulo = a.get_text(strip=True)","                enlace = a.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # 4. Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # 5. Guardar CSV en S3","        final_key = f\"headlines/final/periodico={nombre}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=final_key,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"HTML descargados y procesados correctamente\"}",""],"id":2},{"start":{"row":0,"column":0},"end":{"row":82,"column":0},"action":"insert","lines":["import requests","import boto3","from bs4 import BeautifulSoup","from datetime import datetime","#rrrr","def scrape_headlines(event=None, context=None):","    urls = {","        \"eltiempo\": \"https://www.eltiempo.com/\",","        \"publimetro\": \"https://www.publimetro.co/\"","    }","","    s3 = boto3.client(\"s3\")","    bucket = \"headlinesjune\"  # ← Estaba mal cerrada la comilla","    today = datetime.utcnow().strftime(\"%Y-%m-%d\")","    year, month, day = today.split(\"-\")","","    headers = {","        \"User-Agent\": (","            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"","            \"AppleWebKit/537.36 (KHTML, like Gecko) \"","            \"Chrome/114.0.0.0 Safari/537.36\"","        ),","        \"Accept\": (","            \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\"","        ),","        \"Accept-Language\": \"es-ES,es;q=0.9\",","        \"Referer\": \"https://www.google.com/\",","        \"Connection\": \"keep-alive\",","        \"Upgrade-Insecure-Requests\": \"1\"","    }","","    for nombre, url in urls.items():","        # 1. Descargar HTML","        response = requests.get(url, headers=headers, timeout=10)","        html = response.content.decode(\"utf-8\")","","        # 2. Guardar HTML en S3","        raw_key = f\"headlines/raw/{nombre}-{today}.html\"","        s3.put_object(","            Bucket=bucket,","            Key=raw_key,","            Body=response.content,","            ContentType='text/html'","        )","","        # 3. Procesar con BeautifulSoup","        soup = BeautifulSoup(html, 'html.parser')","        noticias = []","","        if nombre == \"eltiempo\":","            for a in soup.select(\"article a[href]\"):","                titulo = a.get_text(strip=True)","                enlace = a.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # 4. Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # 5. Guardar CSV en S3","        final_key = f\"headlines/final/periodico={nombre}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=final_key,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"HTML descargados y procesados correctamente\"}",""]}]]},"ace":{"folds":[],"scrolltop":900,"scrollleft":0,"selection":{"start":{"row":82,"column":0},"end":{"row":82,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748977059758}