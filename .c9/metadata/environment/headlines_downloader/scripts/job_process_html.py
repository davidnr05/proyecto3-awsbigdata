{"changed":true,"filter":false,"title":"job_process_html.py","tooltip":"/headlines_downloader/scripts/job_process_html.py","value":"import boto3\nimport os\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n##prueba\ns3 = boto3.client(\"s3\")\n\ndef process_headlines(event, context):\n    print(\"ðŸ” Lambda processor.process_headlines activado\")\n\n    keys = []\n\n    # Si se invoca con evento S3\n    if event and 'Records' in event:\n        for record in event['Records']:\n            keys.append(record['s3']['object']['key'])\n\n    # Si se invoca manualmente (por app.py o prueba)\n    elif event and 'keys' in event:\n        keys = event['keys']\n    \n    else:\n        print(\"âš ï¸ No se proporcionaron claves para procesar\")\n        return {\"message\": \"No hay archivos para procesar\"}\n\n    for key in keys:\n        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas estÃ¡tico\n\n        if not key.endswith('.html'):\n            continue\n\n        # Descargar archivo HTML\n        response = s3.get_object(Bucket=bucket, Key=key)\n        html = response['Body'].read().decode('utf-8')\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Extraer nombre del periÃ³dico y fecha\n        filename = os.path.basename(key)\n        nombre_periodico, fecha = filename.replace('.html', '').split('-')\n        year, month, day = fecha.split('-')\n\n        noticias = []\n\n        if nombre_periodico == \"eltiempo\":\n            for article in soup.select(\"article a[href]\"):\n                titulo = article.get_text(strip=True)\n                enlace = article.get(\"href\")\n                if enlace and not enlace.startswith(\"http\"):\n                    enlace = \"https://www.eltiempo.com\" + enlace\n                if titulo and enlace:\n                    noticias.append([\"General\", titulo, enlace])\n\n        elif nombre_periodico == \"publimetro\":\n            for link in soup.select(\"a[href]\"):\n                titulo = link.get_text(strip=True)\n                enlace = link.get(\"href\")\n                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:\n                    if not enlace.startswith(\"http\"):\n                        enlace = \"https://www.publimetro.co\" + enlace\n                    noticias.append([\"General\", titulo, enlace])\n\n        # Crear CSV en memoria\n        csv = \"categoria,titulo,enlace\\n\"\n        for row in noticias:\n            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"\n\n        # Guardar CSV en S3 particionado\n        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"\n        s3.put_object(\n            Bucket=bucket,\n            Key=key_csv,\n            Body=csv.encode(\"utf-8\"),\n            ContentType=\"text/csv\"\n        )\n\n    return {\"message\": \"Procesamiento y guardado de noticias completado\"}\n","undoManager":{"mark":-2,"position":0,"stack":[[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":["#"],"id":2},{"start":{"row":4,"column":1},"end":{"row":4,"column":2},"action":"insert","lines":["#"]},{"start":{"row":4,"column":2},"end":{"row":4,"column":3},"action":"insert","lines":["p"]},{"start":{"row":4,"column":3},"end":{"row":4,"column":4},"action":"insert","lines":["r"]},{"start":{"row":4,"column":4},"end":{"row":4,"column":5},"action":"insert","lines":["u"]},{"start":{"row":4,"column":5},"end":{"row":4,"column":6},"action":"insert","lines":["e"]},{"start":{"row":4,"column":6},"end":{"row":4,"column":7},"action":"insert","lines":["b"]},{"start":{"row":4,"column":7},"end":{"row":4,"column":8},"action":"insert","lines":["a"]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":4,"column":8},"end":{"row":4,"column":8},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748911657420}