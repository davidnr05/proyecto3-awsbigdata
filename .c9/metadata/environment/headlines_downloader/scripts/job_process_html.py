{"changed":true,"filter":false,"title":"job_process_html.py","tooltip":"/headlines_downloader/scripts/job_process_html.py","value":"import boto3\nimport os\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n#yyy\ns3 = boto3.client(\"s3\")\n\ndef process_headlines(event, context):\n    print(\"游대 Lambda processor.process_headlines activado\")\n\n    keys = []\n\n    # Si se invoca con evento S3\n    if event and 'Records' in event:\n        for record in event['Records']:\n            keys.append(record['s3']['object']['key'])\n\n    # Si se invoca manualmente (por app.py o prueba)\n    elif event and 'keys' in event:\n        keys = event['keys']\n    \n    else:\n        print(\"丘멆잺 No se proporcionaron claves para procesar\")\n        return {\"message\": \"No hay archivos para procesar\"}\n\n    for key in keys:\n        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas est치tico\n\n        if not key.endswith('.html'):\n            continue\n\n        # Descargar archivo HTML\n        response = s3.get_object(Bucket=bucket, Key=key)\n        html = response['Body'].read().decode('utf-8')\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Extraer nombre del peri칩dico y fecha\n        filename = os.path.basename(key)\n        nombre_periodico, fecha = filename.replace('.html', '').split('-')\n        year, month, day = fecha.split('-')\n\n        noticias = []\n\n        if nombre_periodico == \"eltiempo\":\n            for article in soup.select(\"article a[href]\"):\n                titulo = article.get_text(strip=True)\n                enlace = article.get(\"href\")\n                if enlace and not enlace.startswith(\"http\"):\n                    enlace = \"https://www.eltiempo.com\" + enlace\n                # Extraer categor칤a desde el enlace\n                categoria = enlace.split(\"/\")[1] if \"/\" in enlace else \"General\"\n                if titulo and enlace:\n                    noticias.append([categoria.capitalize(), titulo, enlace])\n\n        elif nombre_periodico == \"publimetro\":\n            for link in soup.select(\"a[href]\"):\n                titulo = link.get_text(strip=True)\n                enlace = link.get(\"href\")\n                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:\n                    if not enlace.startswith(\"http\"):\n                        enlace = \"https://www.publimetro.co\" + enlace\n                    categoria = enlace.split(\"/\")[3] if enlace.count(\"/\") > 3 else \"General\"\n                    noticias.append([categoria.capitalize(), titulo, enlace])\n\n        # Crear CSV en memoria\n        csv = \"categoria,titulo,enlace\\n\"\n        for row in noticias:\n            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"\n\n        # Guardar CSV en S3 particionado\n        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"\n        s3.put_object(\n            Bucket=bucket,\n            Key=key_csv,\n            Body=csv.encode(\"utf-8\"),\n            ContentType=\"text/csv\"\n        )\n\n    return {\"message\": \"Procesamiento y guardado de noticias completado\"}\n","undoManager":{"mark":-2,"position":1,"stack":[[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":["#"],"id":2},{"start":{"row":4,"column":1},"end":{"row":4,"column":2},"action":"insert","lines":["#"]},{"start":{"row":4,"column":2},"end":{"row":4,"column":3},"action":"insert","lines":["p"]},{"start":{"row":4,"column":3},"end":{"row":4,"column":4},"action":"insert","lines":["r"]},{"start":{"row":4,"column":4},"end":{"row":4,"column":5},"action":"insert","lines":["u"]},{"start":{"row":4,"column":5},"end":{"row":4,"column":6},"action":"insert","lines":["e"]},{"start":{"row":4,"column":6},"end":{"row":4,"column":7},"action":"insert","lines":["b"]},{"start":{"row":4,"column":7},"end":{"row":4,"column":8},"action":"insert","lines":["a"]}],[{"start":{"row":0,"column":0},"end":{"row":76,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","##prueba","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"游대 Lambda processor.process_headlines activado\")","","    keys = []","","    # Si se invoca con evento S3","    if event and 'Records' in event:","        for record in event['Records']:","            keys.append(record['s3']['object']['key'])","","    # Si se invoca manualmente (por app.py o prueba)","    elif event and 'keys' in event:","        keys = event['keys']","    ","    else:","        print(\"丘멆잺 No se proporcionaron claves para procesar\")","        return {\"message\": \"No hay archivos para procesar\"}","","    for key in keys:","        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas est치tico","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri칩dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":3},{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","#yyy","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"游대 Lambda processor.process_headlines activado\")","","    keys = []","","    # Si se invoca con evento S3","    if event and 'Records' in event:","        for record in event['Records']:","            keys.append(record['s3']['object']['key'])","","    # Si se invoca manualmente (por app.py o prueba)","    elif event and 'keys' in event:","        keys = event['keys']","    ","    else:","        print(\"丘멆잺 No se proporcionaron claves para procesar\")","        return {\"message\": \"No hay archivos para procesar\"}","","    for key in keys:","        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas est치tico","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri칩dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                # Extraer categor칤a desde el enlace","                categoria = enlace.split(\"/\")[1] if \"/\" in enlace else \"General\"","                if titulo and enlace:","                    noticias.append([categoria.capitalize(), titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    categoria = enlace.split(\"/\")[3] if enlace.count(\"/\") > 3 else \"General\"","                    noticias.append([categoria.capitalize(), titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}]]},"ace":{"folds":[],"scrolltop":780,"scrollleft":0,"selection":{"start":{"row":79,"column":0},"end":{"row":79,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748911657420}