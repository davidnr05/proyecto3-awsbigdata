{"filter":false,"title":"processor.py","tooltip":"/headlines_downloader/processor.py","undoManager":{"mark":22,"position":22,"stack":[[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer periódico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categorías visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.elespectador.com\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":24}],[{"start":{"row":46,"column":30},"end":{"row":46,"column":58},"action":"remove","lines":["https://www.elespectador.com"],"id":25},{"start":{"row":46,"column":30},"end":{"row":46,"column":56},"action":"insert","lines":["https://www.publimetro.co/"]}],[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer periódico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categorías visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.publimetro.co/\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":26},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":27},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":28},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":29},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":8,"column":4},"end":{"row":8,"column":59},"action":"insert","lines":["print(\"🔁 Lambda processor.process_headlines activado\")"],"id":30}],[{"start":{"row":0,"column":0},"end":{"row":62,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"🔁 Lambda processor.process_headlines activado\")","    ","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":31},{"start":{"row":0,"column":0},"end":{"row":76,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"🔁 Lambda processor.process_headlines activado\")","","    keys = []","","    # Si se invoca con evento S3","    if event and 'Records' in event:","        for record in event['Records']:","            keys.append(record['s3']['object']['key'])","","    # Si se invoca manualmente (por app.py o prueba)","    elif event and 'keys' in event:","        keys = event['keys']","    ","    else:","        print(\"⚠️ No se proporcionaron claves para procesar\")","        return {\"message\": \"No hay archivos para procesar\"}","","    for key in keys:","        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas estático","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":["#"],"id":32},{"start":{"row":4,"column":1},"end":{"row":4,"column":2},"action":"insert","lines":["y"]}],[{"start":{"row":4,"column":2},"end":{"row":4,"column":3},"action":"insert","lines":["y"],"id":33}],[{"start":{"row":49,"column":16},"end":{"row":50,"column":64},"action":"remove","lines":["if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])"],"id":35},{"start":{"row":49,"column":16},"end":{"row":52,"column":69},"action":"insert","lines":["# Extraer categoría desde el enlace","        categoria = enlace.split(\"/\")[1] if \"/\" in enlace else \"General\"","        if titulo and enlace:","            noticias.append([categoria.capitalize(), titulo, enlace])"]}],[{"start":{"row":50,"column":8},"end":{"row":50,"column":12},"action":"insert","lines":["    "],"id":36}],[{"start":{"row":50,"column":12},"end":{"row":50,"column":16},"action":"insert","lines":["    "],"id":37}],[{"start":{"row":51,"column":8},"end":{"row":51,"column":12},"action":"insert","lines":["    "],"id":38}],[{"start":{"row":51,"column":12},"end":{"row":51,"column":16},"action":"insert","lines":["    "],"id":39}],[{"start":{"row":52,"column":0},"end":{"row":52,"column":4},"action":"insert","lines":["    "],"id":40}],[{"start":{"row":52,"column":16},"end":{"row":52,"column":20},"action":"insert","lines":["    "],"id":41}],[{"start":{"row":61,"column":20},"end":{"row":61,"column":64},"action":"remove","lines":["noticias.append([\"General\", titulo, enlace])"],"id":42},{"start":{"row":61,"column":20},"end":{"row":62,"column":69},"action":"insert","lines":["categoria = enlace.split(\"/\")[3] if enlace.count(\"/\") > 3 else \"General\"","            noticias.append([categoria.capitalize(), titulo, enlace])"]}],[{"start":{"row":62,"column":12},"end":{"row":62,"column":16},"action":"insert","lines":["    "],"id":43}],[{"start":{"row":62,"column":16},"end":{"row":62,"column":20},"action":"insert","lines":["    "],"id":44}],[{"start":{"row":62,"column":20},"end":{"row":62,"column":24},"action":"insert","lines":["    "],"id":45}],[{"start":{"row":62,"column":20},"end":{"row":62,"column":24},"action":"remove","lines":["    "],"id":46}],[{"start":{"row":4,"column":3},"end":{"row":4,"column":4},"action":"insert","lines":["y"],"id":47}]]},"ace":{"folds":[],"scrolltop":840,"scrollleft":0,"selection":{"start":{"row":0,"column":0},"end":{"row":79,"column":0},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":59,"state":"start","mode":"ace/mode/python"}},"timestamp":1748975774411,"hash":"8097162d5aad99ec4020b9ebfa330efa4f33620f"}