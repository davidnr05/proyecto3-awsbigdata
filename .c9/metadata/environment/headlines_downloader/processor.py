{"filter":false,"title":"processor.py","tooltip":"/headlines_downloader/processor.py","undoManager":{"mark":7,"position":7,"stack":[[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer periódico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categorías visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.elespectador.com\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":4}],[{"start":{"row":46,"column":30},"end":{"row":46,"column":58},"action":"remove","lines":["https://www.elespectador.com"],"id":5},{"start":{"row":46,"column":30},"end":{"row":46,"column":56},"action":"insert","lines":["https://www.publimetro.co/"]}],[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer periódico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categorías visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.publimetro.co/\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":6},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":7},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":9},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":11},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":8,"column":4},"end":{"row":8,"column":59},"action":"insert","lines":["print(\"🔁 Lambda processor.process_headlines activado\")"],"id":12}],[{"start":{"row":0,"column":0},"end":{"row":62,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"🔁 Lambda processor.process_headlines activado\")","    ","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":13},{"start":{"row":0,"column":0},"end":{"row":76,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\"🔁 Lambda processor.process_headlines activado\")","","    keys = []","","    # Si se invoca con evento S3","    if event and 'Records' in event:","        for record in event['Records']:","            keys.append(record['s3']['object']['key'])","","    # Si se invoca manualmente (por app.py o prueba)","    elif event and 'keys' in event:","        keys = event['keys']","    ","    else:","        print(\"⚠️ No se proporcionaron claves para procesar\")","        return {\"message\": \"No hay archivos para procesar\"}","","    for key in keys:","        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas estático","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del periódico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":68,"column":22},"end":{"row":68,"column":22},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748836478360,"hash":"9346fa29f67e6b7fd9de6ccdaf7add36ac1eeccd"}