{"changed":true,"filter":false,"title":"processor.py","tooltip":"/headlines_downloader/processor.py","value":"import boto3\nimport os\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\n#y\ns3 = boto3.client(\"s3\")\n\ndef process_headlines(event, context):\n    print(\" Lambda processor.process_headlines activado\")\n\n    keys = []\n\n    # Si se invoca con evento S3\n    if event and 'Records' in event:\n        for record in event['Records']:\n            keys.append(record['s3']['object']['key'])\n\n    # Si se invoca manualmente (por app.py o prueba)\n    elif event and 'keys' in event:\n        keys = event['keys']\n    \n    else:\n        print(\"锔 No se proporcionaron claves para procesar\")\n        return {\"message\": \"No hay archivos para procesar\"}\n\n    for key in keys:\n        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas est谩tico\n\n        if not key.endswith('.html'):\n            continue\n\n        # Descargar archivo HTML\n        response = s3.get_object(Bucket=bucket, Key=key)\n        html = response['Body'].read().decode('utf-8')\n        soup = BeautifulSoup(html, 'html.parser')\n\n        # Extraer nombre del peri贸dico y fecha\n        filename = os.path.basename(key)\n        nombre_periodico, fecha = filename.replace('.html', '').split('-')\n        year, month, day = fecha.split('-')\n\n        noticias = []\n\n        if nombre_periodico == \"eltiempo\":\n            for article in soup.select(\"article a[href]\"):\n                titulo = article.get_text(strip=True)\n                enlace = article.get(\"href\")\n                if enlace and not enlace.startswith(\"http\"):\n                    enlace = \"https://www.eltiempo.com\" + enlace\n                if titulo and enlace:\n                    noticias.append([\"General\", titulo, enlace])\n\n        elif nombre_periodico == \"publimetro\":\n            for link in soup.select(\"a[href]\"):\n                titulo = link.get_text(strip=True)\n                enlace = link.get(\"href\")\n                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:\n                    if not enlace.startswith(\"http\"):\n                        enlace = \"https://www.publimetro.co\" + enlace\n                    noticias.append([\"General\", titulo, enlace])\n\n        # Crear CSV en memoria\n        csv = \"categoria,titulo,enlace\\n\"\n        for row in noticias:\n            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"\n\n        # Guardar CSV en S3 particionado\n        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"\n        s3.put_object(\n            Bucket=bucket,\n            Key=key_csv,\n            Body=csv.encode(\"utf-8\"),\n            ContentType=\"text/csv\"\n        )\n\n    return {\"message\": \"Procesamiento y guardado de noticias completado\"}\n","undoManager":{"mark":7,"position":8,"stack":[[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"insert","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer peri贸dico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categor铆as visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.elespectador.com\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":4}],[{"start":{"row":46,"column":30},"end":{"row":46,"column":58},"action":"remove","lines":["https://www.elespectador.com"],"id":5},{"start":{"row":46,"column":30},"end":{"row":46,"column":56},"action":"insert","lines":["https://www.publimetro.co/"]}],[{"start":{"row":0,"column":0},"end":{"row":61,"column":0},"action":"remove","lines":["import boto3","import os","import csv","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        # Descargar el archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html_content = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html_content, 'html.parser')","","        # Extraer peri贸dico y fecha del nombre del archivo","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            # Extrae noticias destacadas de El Tiempo","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"elespectador\":","            # Extrae noticias principales y categor铆as visibles","            for card in soup.select(\"section article\"):","                categoria_tag = card.select_one(\"span\")","                titulo_tag = card.select_one(\"a[href]\")","","                categoria = categoria_tag.get_text(strip=True) if categoria_tag else \"General\"","                titulo = titulo_tag.get_text(strip=True) if titulo_tag else \"\"","                enlace = titulo_tag.get(\"href\") if titulo_tag else \"\"","","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.publimetro.co/\" + enlace","","                if titulo and enlace:","                    noticias.append([categoria, titulo, enlace])","","        # Crear CSV","        csv_content = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv_content += \",\".join([f'\"{cell}\"' for cell in row]) + \"\\n\"","","        # Guardar en S3 particionado","        new_key = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(Bucket=bucket, Key=new_key, Body=csv_content.encode('utf-8'), ContentType='text/csv')","","    return {\"message\": \"Noticias procesadas y guardadas en CSV\"}",""],"id":6},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri贸dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri贸dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":7},{"start":{"row":0,"column":0},"end":{"row":60,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri贸dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":9},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":7,"column":38},"end":{"row":8,"column":0},"action":"insert","lines":["",""],"id":11},{"start":{"row":8,"column":0},"end":{"row":8,"column":4},"action":"insert","lines":["    "]}],[{"start":{"row":8,"column":4},"end":{"row":8,"column":59},"action":"insert","lines":["print(\" Lambda processor.process_headlines activado\")"],"id":12}],[{"start":{"row":0,"column":0},"end":{"row":62,"column":0},"action":"remove","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\" Lambda processor.process_headlines activado\")","    ","    for record in event['Records']:","        bucket = record['s3']['bucket']['name']","        key = record['s3']['object']['key']","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri贸dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""],"id":13},{"start":{"row":0,"column":0},"end":{"row":76,"column":0},"action":"insert","lines":["import boto3","import os","from bs4 import BeautifulSoup","from datetime import datetime","","s3 = boto3.client(\"s3\")","","def process_headlines(event, context):","    print(\" Lambda processor.process_headlines activado\")","","    keys = []","","    # Si se invoca con evento S3","    if event and 'Records' in event:","        for record in event['Records']:","            keys.append(record['s3']['object']['key'])","","    # Si se invoca manualmente (por app.py o prueba)","    elif event and 'keys' in event:","        keys = event['keys']","    ","    else:","        print(\"锔 No se proporcionaron claves para procesar\")","        return {\"message\": \"No hay archivos para procesar\"}","","    for key in keys:","        bucket = \"headlinesjune\"  # Reemplaza por tu bucket si lo necesitas est谩tico","","        if not key.endswith('.html'):","            continue","","        # Descargar archivo HTML","        response = s3.get_object(Bucket=bucket, Key=key)","        html = response['Body'].read().decode('utf-8')","        soup = BeautifulSoup(html, 'html.parser')","","        # Extraer nombre del peri贸dico y fecha","        filename = os.path.basename(key)","        nombre_periodico, fecha = filename.replace('.html', '').split('-')","        year, month, day = fecha.split('-')","","        noticias = []","","        if nombre_periodico == \"eltiempo\":","            for article in soup.select(\"article a[href]\"):","                titulo = article.get_text(strip=True)","                enlace = article.get(\"href\")","                if enlace and not enlace.startswith(\"http\"):","                    enlace = \"https://www.eltiempo.com\" + enlace","                if titulo and enlace:","                    noticias.append([\"General\", titulo, enlace])","","        elif nombre_periodico == \"publimetro\":","            for link in soup.select(\"a[href]\"):","                titulo = link.get_text(strip=True)","                enlace = link.get(\"href\")","                if enlace and titulo and \"/\" in enlace and len(titulo) > 40:","                    if not enlace.startswith(\"http\"):","                        enlace = \"https://www.publimetro.co\" + enlace","                    noticias.append([\"General\", titulo, enlace])","","        # Crear CSV en memoria","        csv = \"categoria,titulo,enlace\\n\"","        for row in noticias:","            csv += \",\".join(f'\"{r}\"' for r in row) + \"\\n\"","","        # Guardar CSV en S3 particionado","        key_csv = f\"headlines/final/periodico={nombre_periodico}/year={year}/month={month}/day={day}/noticias.csv\"","        s3.put_object(","            Bucket=bucket,","            Key=key_csv,","            Body=csv.encode(\"utf-8\"),","            ContentType=\"text/csv\"","        )","","    return {\"message\": \"Procesamiento y guardado de noticias completado\"}",""]}],[{"start":{"row":4,"column":0},"end":{"row":4,"column":1},"action":"insert","lines":["#"],"id":14},{"start":{"row":4,"column":1},"end":{"row":4,"column":2},"action":"insert","lines":["y"]}]]},"ace":{"folds":[],"scrolltop":0,"scrollleft":0,"selection":{"start":{"row":4,"column":2},"end":{"row":4,"column":2},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":0},"timestamp":1748836478360}